---
id: 1
title: qdrant-cloud-integration-plan
stage: plan
date: 2025-12-20
surface: agent
model: claude-sonnet-4-5-20250929
feature: qdrant-cloud-integration
branch: 1-qdrant-cloud-integration
user: Claude Code User
command: /sp.plan
labels: ["qdrant", "cloud", "vector-database", "rag", "integration", "architecture"]
links:
  spec: specs/1-qdrant-cloud-integration/spec.md
  ticket: null
  adr: null
  pr: null
files:
 - specs/1-qdrant-cloud-integration/plan.md
 - specs/1-qdrant-cloud-integration/research.md
 - specs/1-qdrant-cloud-integration/data-model.md
 - specs/1-qdrant-cloud-integration/quickstart.md
 - specs/1-qdrant-cloud-integration/contracts/ingestion-contract.md
 - specs/1-qdrant-cloud-integration/contracts/search-contract.md
 - specs/1-qdrant-cloud-integration/contracts/chat-contract.md
tests:
 - specs/1-qdrant-cloud-integration/data-model.md
 - specs/1-qdrant-cloud-integration/contracts/ingestion-contract.md
 - specs/1-qdrant-cloud-integration/contracts/search-contract.md
 - specs/1-qdrant-cloud-integration/contracts/chat-contract.md
---

## Prompt

$ARGUMENTS

## Response snapshot

# Implementation Plan: Qdrant Cloud Integration

**Branch**: `1-qdrant-cloud-integration` | **Date**: 2025-12-20 | **Spec**: [link](specs/1-qdrant-cloud-integration/spec.md)
**Input**: Feature specification from `/specs/1-qdrant-cloud-integration/spec.md`

**Note**: This template is filled in by the `/sp.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Migration from current vector storage to Qdrant Cloud for the RAG-based ChatKit chatbot. The system will use Qdrant Cloud (URL + API key authentication) to store embeddings generated by the Gemini embedding model, implement semantic search functionality, and ensure chatbot responses are strictly based on book content from the `docs/` folder.

## Technical Context

**Language/Version**: Python 3.11
**Primary Dependencies**: FastAPI, Qdrant Cloud client, Google Generative AI (Gemini), ChatKit SDK
**Storage**: Qdrant Cloud (vector database), existing file system for docs/ folder
**Testing**: pytest
**Target Platform**: Linux server
**Project Type**: Web backend service
**Performance Goals**: 95% accuracy in retrieving relevant book content, response time under 3 seconds
**Constraints**: <200ms p95 for Qdrant search operations, maintain compatibility with existing frontend and ChatKit setup
**Scale/Scope**: Support concurrent users querying book content, handle book content of various sizes

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

Based on the constitution file, this implementation plan aligns with:
- Technical Requirements: Uses Qdrant Cloud as specified in the constitution (section 35)
- Intelligence Accumulation: The vector database integration will create reusable intelligence for the RAG system
- Right Altitude: Maintains appropriate level of abstraction without being too low or too high
- Meta-Awareness Against Convergence: The implementation will avoid predictable patterns by using proper RAG architecture

**Post-Design Re-evaluation**:
- All research findings confirm alignment with constitution
- Qdrant Cloud integration supports the embedded RAG chatbot requirement
- Implementation maintains the right altitude with appropriate abstraction levels
- Design decisions support intelligence accumulation across the system

## Project Structure

### Documentation (this feature)

```text
specs/1-qdrant-cloud-integration/
â”œâ”€â”€ plan.md              # This file (/sp.plan command output)
â”œâ”€â”€ research.md          # Phase 0 output (/sp.plan command)
â”œâ”€â”€ data-model.md        # Phase 1 output (/sp.plan command)
â”œâ”€â”€ quickstart.md        # Phase 1 output (/sp.plan command)
â”œâ”€â”€ contracts/           # Phase 1 output (/sp.plan command)
â””â”€â”€ tasks.md             # Phase 2 output (/sp.tasks command - NOT created by /sp.plan)
```

### Source Code (repository root)

```text
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”œâ”€â”€ document_chunk.py
â”‚   â”‚   â””â”€â”€ vector_embedding.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ vector_db_service.py
â”‚   â”‚   â”œâ”€â”€ ingestion_service.py
â”‚   â”‚   â””â”€â”€ chat_service.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ ingestion_api.py
â”‚   â”‚   â””â”€â”€ chat_api.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ file_parser.py
â”‚       â””â”€â”€ text_splitter.py
â””â”€â”€ tests/
    â”œâ”€â”€ unit/
    â”œâ”€â”€ integration/
    â””â”€â”€ contract/
```

**Structure Decision**: Selected web application structure with dedicated backend for the RAG system, maintaining compatibility with existing frontend and ChatKit setup.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |

# Research: Qdrant Cloud Integration

## Decision: Qdrant Cloud Vector Database Selection
**Rationale**: Qdrant Cloud was specified in both the user requirements and project constitution as the target vector database. It provides managed vector storage with semantic search capabilities needed for the RAG system. The cloud offering provides scalability, reliability, and reduced operational overhead compared to self-hosted solutions.

## Decision: Qdrant Python Client Library
**Rationale**: The official `qdrant-client` Python library provides the necessary functionality to interact with Qdrant Cloud, including:
- Collection management
- Vector storage and retrieval
- Semantic search with configurable similarity metrics
- Payload storage for metadata
- API key authentication

**Alternatives considered**:
- Pinecone: Proprietary solution with potential vendor lock-in
- Weaviate: Alternative open-source vector database but not specified in constitution
- Self-hosted vector stores: Increased operational complexity

## Decision: Document Chunking Strategy
**Rationale**: For effective RAG with book content, documents need to be chunked into smaller segments that preserve semantic meaning while fitting within LLM context windows. Using a sliding window approach with overlap ensures semantic continuity while maintaining search effectiveness.

**Parameters**:
- Chunk size: 1000 tokens (approximately 500-800 words)
- Overlap: 200 tokens to maintain context
- Metadata preservation: Include source document and position information

## Decision: Embedding Generation Process
**Rationale**: Using the existing Gemini embedding model as specified in requirements maintains consistency with the current system. The embeddings will be generated for each document chunk and stored alongside the text in Qdrant for semantic search.

**Process**:
1. Parse documents from `docs/` folder
2. Split into chunks
3. Generate embeddings using Gemini model
4. Store in Qdrant with metadata

## Decision: Qdrant Collection Schema
**Rationale**: The collection schema needs to support efficient semantic search while preserving document context for the chatbot.

**Schema**:
- Vector: Embedding from Gemini model
- Payload:
  - `content`: The chunk text
  - `source_document`: Original file name
  - `chunk_id`: Sequential identifier
  - `position`: Position in original document
  - `metadata`: Additional document metadata

## Decision: Error Handling Strategy
**Rationale**: Robust error handling is essential for production systems, especially when depending on external services like Qdrant Cloud.

**Approach**:
- Connection retries with exponential backoff
- Graceful degradation when Qdrant is unavailable
- Fallback responses ("Not found in the book") when retrieval fails
- Comprehensive logging for debugging

## Decision: Semantic Search Configuration
**Rationale**: The search parameters need to balance precision and recall for book content retrieval.

**Configuration**:
- Similarity metric: Cosine similarity (standard for text embeddings)
- Number of results: 3-5 most relevant chunks
- Score threshold: Minimum similarity score to avoid low-quality matches
- Payload inclusion: Return full content for context injection

# Data Model: Qdrant Cloud Integration

## Document Chunk Entity

**Description**: A segment of book content with associated metadata and embedding vector for semantic search

**Fields**:
- `id` (string): Unique identifier for the chunk (UUID)
- `content` (string): The text content of the chunk
- `source_document` (string): Name/path of the original document
- `chunk_id` (integer): Sequential identifier within the source document
- `position` (integer): Position offset in the original document
- `embedding_vector` (list[float]): Vector representation generated by Gemini model
- `metadata` (dict): Additional document metadata (author, creation date, etc.)
- `created_at` (datetime): Timestamp when the chunk was created
- `updated_at` (datetime): Timestamp when the chunk was last updated

**Validation Rules**:
- `content` must not be empty
- `embedding_vector` must have consistent dimensions (based on Gemini model output)
- `source_document` must reference a valid document in the `docs/` folder
- `chunk_id` must be unique within the source document

**Relationships**:
- Belongs to one `source_document`
- Multiple chunks can belong to the same source document

## Qdrant Collection Entity

**Description**: Container in Qdrant Cloud storing document chunks with embeddings

**Fields**:
- `name` (string): Name of the Qdrant collection
- `vector_size` (integer): Dimension of the embedding vectors
- `distance_metric` (string): Distance metric used for similarity search (cosine, euclidean, etc.)
- `chunks` (list[Document Chunk]): List of document chunks stored in the collection

**Validation Rules**:
- `vector_size` must match the Gemini model's embedding dimension
- `distance_metric` must be supported by Qdrant
- Collection name must be unique in the Qdrant Cloud instance

**Relationships**:
- Contains many `Document Chunk` entities

## Embedding Vector Entity

**Description**: Numerical representation of text content generated by the Gemini model

**Fields**:
- `vector_data` (list[float]): The actual embedding vector values
- `model_version` (string): Version of the Gemini model used
- `dimension` (integer): Number of dimensions in the vector
- `source_text_hash` (string): Hash of the original text for integrity verification

**Validation Rules**:
- `vector_data` must have consistent dimensions
- `dimension` must match the expected model output
- `source_text_hash` must correspond to the actual source text

**Relationships**:
- Associated with one `Document Chunk`
- Used by semantic search operations

## Search Query Entity

**Description**: User input converted to embedding for semantic similarity search

**Fields**:
- `query_text` (string): Original user query text
- `query_embedding` (list[float]): Embedding vector of the query
- `search_params` (dict): Parameters for the similarity search (top_k, score_threshold, etc.)
- `timestamp` (datetime): When the query was processed

**Validation Rules**:
- `query_text` must not be empty
- `query_embedding` must have consistent dimensions
- `search_params` must contain valid Qdrant search parameters

**Relationships**:
- Generates results from `Qdrant Collection`
- Associated with retrieved `Document Chunk` entities

## State Transitions

### Document Chunk States
- `CREATED`: Chunk has been created but not yet processed
- `EMBEDDED`: Embedding has been generated for the chunk
- `STORED`: Chunk has been stored in Qdrant Cloud
- `ACTIVE`: Chunk is available for search operations
- `DEPRECATED`: Chunk is no longer current (when source document is updated)

### Search Query States
- `RECEIVED`: Query has been received from user
- `EMBEDDED`: Query text has been converted to embedding
- `SEARCHED`: Similarity search has been performed
- `RESULTS_READY`: Relevant chunks have been retrieved
- `ANSWER_GENERATED`: Final response has been generated

# Quickstart: Qdrant Cloud Integration

## Overview
This guide provides a quick setup for integrating Qdrant Cloud into your RAG-based ChatKit chatbot system. The integration will replace the current vector storage with Qdrant Cloud while maintaining compatibility with existing frontend and ChatKit setup.

## Prerequisites
- Python 3.11+
- Existing Gemini embedding model configuration
- Qdrant Cloud account and API key
- Book content in the `docs/` folder

## Installation
```bash
pip install qdrant-client google-generativeai fastapi
```

## Configuration
1. Set up environment variables:
```bash
export QDRANT_URL="your-qdrant-cloud-url"
export QDRANT_API_KEY="your-api-key"
export GEMINI_API_KEY="your-gemini-api-key"
```

2. Configure the Qdrant client in your application:
```python
from qdrant_client import QdrantClient

client = QdrantClient(
    url=os.getenv("QDRANT_URL"),
    api_key=os.getenv("QDRANT_API_KEY"),
    prefer_grpc=True  # Optional: for better performance
)
```

## Core Implementation Steps

### 1. Document Ingestion
```python
from services.ingestion_service import IngestionService

# Initialize the ingestion service
ingestion_service = IngestionService(qdrant_client=client, gemini_model="embedding-001")

# Process all documents in the docs/ folder
ingestion_service.ingest_documents_from_folder("docs/")
```

### 2. Semantic Search
```python
from services.vector_db_service import VectorDBService

# Initialize the vector database service
vector_service = VectorDBService(qdrant_client=client)

# Perform semantic search
search_results = vector_service.search_similar_chunks(query_text="your query", top_k=5)
```

### 3. Chat Integration
```python
from services.chat_service import ChatService

# Initialize the chat service with vector retrieval
chat_service = ChatService(
    vector_service=vector_service,
    gemini_client=gemini_client
)

# Get response based on retrieved context
response = chat_service.get_rag_response(user_query="What does the book say about AI?", top_k=3)
```

## Key Components

### Ingestion Service (`ingestion_service.py`)
- Reads documents from `docs/` folder
- Splits documents into semantic chunks
- Generates embeddings using Gemini model
- Stores chunks with embeddings in Qdrant Cloud

### Vector DB Service (`vector_db_service.py`)
- Manages interactions with Qdrant Cloud
- Performs semantic search operations
- Handles error cases and fallbacks
- Maintains connection pooling for efficiency

### Chat Service (`chat_service.py`)
- Integrates vector retrieval with chatbot logic
- Ensures responses are grounded in book content
- Returns "Not found in the book" when no relevant content exists
- Maintains compatibility with existing ChatKit setup

## Environment Variables
- `QDRANT_URL`: Your Qdrant Cloud instance URL
- `QDRANT_API_KEY`: Your Qdrant Cloud API key
- `GEMINI_API_KEY`: Your Google Gemini API key
- `DOCS_FOLDER`: Path to book content (defaults to "docs/")

## Error Handling
The system includes robust error handling:
- Connection retries with exponential backoff
- Graceful degradation when Qdrant is unavailable
- Fallback responses ("Not found in the book") when retrieval fails
- Comprehensive logging for debugging

## Testing
Run the following to verify the integration:
```bash
# Unit tests
pytest tests/unit/

# Integration tests
pytest tests/integration/

# Contract tests
pytest tests/contract/
```

## Next Steps
1. Review the API contracts in the `contracts/` directory
2. Implement the data models as defined in `data-model.md`
3. Follow the tasks in `tasks.md` for complete implementation

# API Contract: Document Ingestion Service

## Overview
Contract for the document ingestion service that processes book content from the `docs/` folder and stores embeddings in Qdrant Cloud.

## Endpoints

### POST /api/ingest
**Description**: Process all documents in the docs/ folder and store their embeddings in Qdrant Cloud

**Request**:
```
POST /api/ingest
Content-Type: application/json
Authorization: Bearer {token}

{
  "docs_folder": "docs/",  // Optional: defaults to "docs/"
  "force_reprocess": false // Optional: if true, reprocesses all documents
}
```

**Response**:
- `200 OK`: Ingestion completed successfully
```
{
  "status": "completed",
  "processed_documents": 15,
  "processed_chunks": 120,
  "timestamp": "2025-12-20T10:30:00Z",
  "collection_name": "book_content"
}
```

- `400 Bad Request`: Invalid request parameters
```
{
  "error": "InvalidParameter",
  "message": "docs_folder path does not exist"
}
```

- `500 Internal Server Error`: Processing failed
```
{
  "error": "ProcessingFailed",
  "message": "Failed to connect to Qdrant Cloud"
}
```

**Authentication**: Required - API key or JWT token

**Authorization**: Admin or system user role required

---

### GET /api/ingest/status
**Description**: Get the status of the ingestion process

**Request**:
```
GET /api/ingest/status
Authorization: Bearer {token}
```

**Response**:
- `200 OK`: Status retrieved successfully
```
{
  "status": "processing|completed|failed",
  "progress": 0.75,  // Percentage complete
  "total_documents": 20,
  "processed_documents": 15,
  "total_chunks": 150,
  "stored_chunks": 112,
  "last_updated": "2025-12-20T10:25:00Z"
}
```

---

### POST /api/ingest/single
**Description**: Process a single document and store its embeddings in Qdrant Cloud

**Request**:
```
POST /api/ingest/single
Content-Type: application/json
Authorization: Bearer {token}

{
  "document_path": "docs/chapter1.md",
  "metadata": {
    "author": "Book Author",
    "title": "Chapter 1 Title"
  }
}
```

**Response**:
- `200 OK`: Document processed successfully
```
{
  "status": "completed",
  "document_path": "docs/chapter1.md",
  "processed_chunks": 8,
  "timestamp": "2025-12-20T10:30:00Z",
  "collection_name": "book_content"
}
```

- `400 Bad Request`: Document not found or invalid format
```
{
  "error": "DocumentError",
  "message": "Document not found at path docs/chapter1.md"
}
```

## Error Codes

| Code | Meaning |
|------|---------|
| `INGESTION_NOT_STARTED` | Ingestion process has not started |
| `INGESTION_IN_PROGRESS` | Ingestion is currently running |
| `QDRANT_CONNECTION_FAILED` | Unable to connect to Qdrant Cloud |
| `EMBEDDING_GENERATION_FAILED` | Failed to generate embeddings |
| `DOCUMENT_PARSING_FAILED` | Unable to parse the document format |
| `INSUFFICIENT_CONTENT` | Document has no meaningful text content |
| `QDRANT_STORAGE_FAILED` | Failed to store in Qdrant Cloud |

## Security Considerations
- Only authorized users can trigger ingestion
- Document paths must be validated to prevent directory traversal
- API rate limits should be implemented to prevent abuse

# API Contract: Semantic Search Service

## Overview
Contract for the semantic search service that performs similarity search against Qdrant Cloud to retrieve relevant book content chunks.

## Endpoints

### POST /api/search
**Description**: Perform semantic search against stored book content using Qdrant Cloud

**Request**:
```
POST /api/search
Content-Type: application/json
Authorization: Bearer {token}

{
  "query": "What does the book say about artificial intelligence?",
  "top_k": 5,           // Optional: number of results to return (default: 3)
  "score_threshold": 0.5 // Optional: minimum similarity score (default: 0.3)
}
```

**Response**:
- `200 OK`: Search completed successfully
```
{
  "query": "What does the book say about artificial intelligence?",
  "results": [
    {
      "id": "chunk_12345",
      "content": "Artificial intelligence is a branch of computer science...",
      "source_document": "docs/chapter3.md",
      "chunk_id": 1,
      "position": 1500,
      "score": 0.87,
      "metadata": {
        "author": "Book Author",
        "title": "Chapter 3 Title"
      }
    },
    {
      "id": "chunk_12346",
      "content": "Machine learning is a subset of artificial intelligence...",
      "source_document": "docs/chapter5.md",
      "chunk_id": 2,
      "position": 800,
      "score": 0.76,
      "metadata": {
        "author": "Book Author",
        "title": "Chapter 5 Title"
      }
    }
  ],
  "search_time_ms": 45,
  "timestamp": "2025-12-20T10:30:00Z"
}
```

- `400 Bad Request`: Invalid search parameters
```
{
  "error": "InvalidParameter",
  "message": "Query cannot be empty"
}
```

- `500 Internal Server Error`: Search failed
```
{
  "error": "SearchFailed",
  "message": "Failed to connect to Qdrant Cloud"
}
```

**Authentication**: Required - API key or JWT token

**Authorization**: Any authenticated user can perform search

---

### GET /api/search/health
**Description**: Check the health of the search service and Qdrant Cloud connection

**Request**:
```
GET /api/search/health
```

**Response**:
- `200 OK`: Service is healthy
```
{
  "status": "healthy",
  "qdrant_connected": true,
  "collection_exists": true,
  "last_heartbeat": "2025-12-20T10:30:00Z",
  "collection_name": "book_content",
  "total_chunks": 120
}
```

- `503 Service Unavailable`: Service is unhealthy
```
{
  "status": "unhealthy",
  "qdrant_connected": false,
  "message": "Cannot connect to Qdrant Cloud"
}
```

## Error Codes

| Code | Meaning |
|------|---------|
| `QDRANT_CONNECTION_FAILED` | Unable to connect to Qdrant Cloud |
| `SEARCH_FAILED` | Semantic search operation failed |
| `EMPTY_QUERY` | Query string is empty or invalid |
| `NO_RESULTS_FOUND` | No relevant content found (not an error, but a valid outcome) |
| `SCORE_BELOW_THRESHOLD` | All results have scores below the threshold |
| `INVALID_PARAMETERS` | Invalid search parameters provided |

## Performance Requirements
- Search response time: <200ms for 95% of requests
- Maximum concurrent searches: 100
- Minimum similarity score: Configurable threshold (default 0.3)

## Security Considerations
- Query content should be sanitized to prevent injection attacks
- API rate limits should be implemented to prevent abuse
- Search results should not expose sensitive metadata unnecessarily

# API Contract: RAG Chat Service

## Overview
Contract for the RAG-based chat service that retrieves relevant book content and generates responses strictly based on book content.

## Endpoints

### POST /api/chat
**Description**: Get a response from the chatbot based on book content using RAG

**Request**:
```
POST /api/chat
Content-Type: application/json
Authorization: Bearer {token}

{
  "message": "What does the book say about artificial intelligence?",
  "context_chunks": 3,      // Optional: number of context chunks to retrieve (default: 3)
  "temperature": 0.1,       // Optional: response randomness (default: 0.1 for factual responses)
  "session_id": "sess_123"  // Optional: for conversation context
}
```

**Response**:
- `200 OK`: Chat response generated successfully
```
{
  "request_id": "req_45678",
  "message": "According to the book, artificial intelligence is a branch of computer science that aims to create software or machines that exhibit human-like intelligence.",
  "retrieved_chunks": 3,
  "sources": [
    {
      "document": "docs/chapter3.md",
      "chunk_id": 1,
      "score": 0.87
    }
  ],
  "timestamp": "2025-12-20T10:30:00Z",
  "session_id": "sess_123"
}
```

- `200 OK`: No relevant content found
```
{
  "request_id": "req_45679",
  "message": "Not found in the book.",
  "retrieved_chunks": 0,
  "sources": [],
  "timestamp": "2025-12-20T10:30:01Z",
  "session_id": "sess_123"
}
```

- `400 Bad Request`: Invalid request parameters
```
{
  "error": "InvalidParameter",
  "message": "Message cannot be empty"
}
```

- `500 Internal Server Error`: Processing failed
```
{
  "error": "ProcessingFailed",
  "message": "Failed to retrieve content from Qdrant Cloud"
}
```

**Authentication**: Required - API key or JWT token

**Authorization**: Any authenticated user can use chat service

---

### POST /api/chat/stream
**Description**: Get a streaming response from the chatbot based on book content using RAG

**Request**:
```
POST /api/chat/stream
Content-Type: application/json
Authorization: Bearer {token}

{
  "message": "Explain the concept of machine learning from the book",
  "context_chunks": 3,
  "session_id": "sess_123"
}
```

**Response**:
- `200 OK`: Streaming response with Content-Type: text/plain
```
data: {"type": "retrieval_start", "timestamp": "2025-12-20T10:30:00Z"}

data: {"type": "retrieval_complete", "retrieved_chunks": 2, "sources": ["docs/chapter4.md", "docs/chapter7.md"], "timestamp": "2025-12-20T10:30:01Z"}

data: {"type": "chunk", "content": "Machine learning is", "timestamp": "2025-12-20T10:30:02Z"}

data: {"type": "chunk", "content": " a subset of artificial intelligence", "timestamp": "2025-12-20T10:30:02Z"}

data: {"type": "chunk", "content": " that focuses on algorithms", "timestamp": "2025-12-20T10:30:03Z"}

data: {"type": "complete", "request_id": "req_45680", "timestamp": "2025-12-20T10:30:03Z"}
```

---

### GET /api/chat/health
**Description**: Check the health of the chat service and its dependencies

**Request**:
```
GET /api/chat/health
```

**Response**:
- `200 OK`: Service is healthy
```
{
  "status": "healthy",
  "qdrant_connected": true,
  "gemini_connected": true,
  "last_heartbeat": "2025-12-20T10:30:00Z",
  "active_sessions": 5
}
```

- `503 Service Unavailable`: Service is unhealthy
```
{
  "status": "unhealthy",
  "qdrant_connected": false,
  "gemini_connected": true,
  "message": "Cannot connect to Qdrant Cloud"
}
```

## Error Codes

| Code | Meaning |
|------|---------|
| `QDRANT_CONNECTION_FAILED` | Unable to connect to Qdrant Cloud for retrieval |
| `GEMINI_CONNECTION_FAILED` | Unable to connect to Gemini API for response generation |
| `NO_RELEVANT_CONTENT` | No relevant content found in book (results in "Not found in the book" response) |
| `RESPONSE_GENERATION_FAILED` | Failed to generate response based on retrieved content |
| `INVALID_MESSAGE` | User message is empty or invalid |
| `SESSION_EXPIRED` | Session ID is no longer valid |

## RAG Behavior Rules
- All responses must be grounded only in retrieved book data
- No hallucinations or outside knowledge allowed
- If no relevant content is found, respond with "Not found in the book."
- Responses should be clear and concise

## Performance Requirements
- Response time: <3 seconds for 95% of requests
- Maximum context length: 4096 tokens to prevent overflow
- Support for streaming responses to improve user experience

## Security Considerations
- Message content should be sanitized to prevent prompt injection
- Session IDs should be securely generated and managed
- Rate limits should be implemented to prevent abuse
- Response content should not expose sensitive document metadata

## Outcome

- âœ… Impact: Created comprehensive architecture plan for Qdrant Cloud integration with complete data models, API contracts, and quickstart guide
- ðŸ§ª Tests: API contracts provide clear specifications for testing validation
- ðŸ“ Files: Generated plan.md, research.md, data-model.md, quickstart.md, and contract files
- ðŸ” Next prompts: Ready for /sp.tasks to generate implementation tasks
- ðŸ§  Reflection: Architecture aligns with project constitution and provides solid foundation for RAG system implementation

## Evaluation notes (flywheel)

- Failure modes observed: None
- Graders run and results (PASS/FAIL): PASS - All architecture components validated
- Prompt variant (if applicable): N/A
- Next experiment (smallest change to try): N/A